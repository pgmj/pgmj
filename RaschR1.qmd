---
title: "Rasch in R and Quarto, part 1"
subtitle: "Intro and WAAQ-7"
title-block-banner: "#009ca6"
author: 
  name: Magnus Johansson
  affiliation: RISE Research Institutes of Sweden
  affiliation-url: https://ri.se/shic
  orcid: 0000-0003-1669-592X
date: '2022-05-10'
format: 
  html:
    toc: true
    self-contained: true
    logo: rise_logo_quarto.png
    mainfont: 'Lato'
    monofont: 'Roboto Mono'
    code-overflow: wrap
    code-tools: true
    code-fold: true
    number-sections: true
    fig-dpi: 300
  pdf:
    papersize: a4
    documentclass: article #article, report or book
    classoption: [twocolumn, portrait]
  revealjs:
    theme: default
    logo: rise_logo_quarto.png
    chalkboard: false
    self-contained: true
#    footer: 'Material skapat av magnus.p.johansson@ri.se'
    mainfont: 'Lato'
    slide-level: 4
    scrollable: true
    smaller: true
execute:
  echo: false
  warning: false
  message: false
#  cache: true
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
bibliography: 
- references.bib
- grateful-refs.bib
---

```{r}
#| label: blurb
#| include: false

### Please check for the latest version of this file at:
### https://github.com/pgmj/pgmj/blob/main/RaschR1.qmd 
### and the companion file with more chunks:
### https://github.com/pgmj/pgmj/blob/main/RaschR2.qmd
```


## Why Rasch analysis in R?

::: incremental
-   Traceability
    -   Transparency in decisionmaking
-   Reproducibility
-   Freely available
-   Documentation in the same file as analysis
    -   (with Quarto/Rmarkdown)
:::

## Agenda

::: incremental
-   Sample analysis with provided data and code
    -   WAAQ-7
    -   PSS-10
-   Suggested structure for analysis
    -   Confirmatory and/or exploratory
:::

## Quarto formatting basics

<https://quarto.org/docs/authoring/markdown-basics.html>

-   Keep the page above open in a web browser
-   We will use library(kableExtra) for tables
    -   since it works with HTML/revealjs and PDF output

```{r}
#| label: setup
#| code-fold: false
#| include: false

library(car)
library(grateful)
library(kableExtra)
library(readxl)
library(tidyverse)
library(eRm)
library(mokken)
library(mirt)
library(psych)
library(ggplot2)
library(psychotree)
library(matrixStats)
library(reshape)
library(knitr)
library(cowplot)

### some commands exist in multiple packages, here we define preferred ones that are frequently used
select <- dplyr::select
count <- dplyr::count
recode <- car::recode
rename <- dplyr::rename

### set up color palette based on RISE guidelines
RISEprimGreen <- "#009ca6"
RISEprimRed <- "#e83c63"
RISEprimYellow <- "#ffe500"
RISEprimGreenMid <- "#8dc8c7"
RISEprimRedMid <- "#f5a9ab"
RISEprimYellowMid <- "#ffee8d"
RISEprimGreenLight <- "#ebf5f0"
RISEprimRedLight <- "#fde8df"
RISEprimYellowLight <- "#fff7dd"
RISEcompPurple <- "#482d55"
RISEcompGreenDark <- "#0e4e65"
RISEgrey1 <- "#f0f0f0"
RISEgrey2 <- "#c8c8c8"
RISEgrey3 <- "#828282"
RISEgrey4 <- "#555555"

# set some colors used later
cutoff_line <- RISEprimRed
dot_color <- "black"
backg_color <- RISEprimGreenLight

# set fontsize for all tables
r.fontsize <- 15

### first we pre-set our chosen cut-off values for some commonly used indices:
msq_min <- 0.7
msq_max <- 1.3
zstd_min <- -2
zstd_max <- 2
loc_dep <- 0.2 # above average residual correlation
dif_dif <- 0.5 # logits difference between groups in average item location (DIF)

### zstd is inflated with large samples (N > 500). Reduce sample size to jz and 
### run analysis yz random samples to get average ZSTD
jz = 300 # number to include in dataset
yz = 10 # number of random samples

# Load data
df<-as.data.frame(read_excel("C:/Users/magnuspjo/RISE/KP Centrum för Kategoriskt Baserade Mätningar - Dokument/Kunskapsutveckling/RaschR/data/GNI23data v1.1.xls"))

### if you need to download the datafile:
# library(readODS)
# url <- "https://figshare.com/ndownloader/files/27953790"
# destfile <- "gnidata.ods"
# download.file(url, destfile, mode="wb")
# df <- read_ods(file = "gnidata.ods")

df <- df %>% 
  mutate_if(is.character, as.numeric)

# Make a backup of the dataframe
df.all<-df

# Select the variables we will work with, which as this point can only be items
df.omit.na <- df %>% 
  select(starts_with("WAAQ"),Sex,Age) %>% # variables that start with "WAAQ"
  na.omit() # remove rows with 100% missing data

# DIF variables into vectors
dif.gender <- df.omit.na$Sex
dif.age <- df.omit.na$Age
# them remove them from dataframe
df.omit.na$Sex <- NULL
df.omit.na$Age <- NULL

# set item names to neutral qX (and avoid issues with _ in pdf output)
names(df.omit.na) <- paste0("q", c(1:ncol(df.omit.na)))

# recode WAAQ items to 0-6 (instead of 1-7)
for (i in 1:7) {
  df.omit.na[,i]<-recode(df.omit.na[,i],"1=0;2=1;3=2;4=3;5=4;6=5;7=6",as.factor=FALSE)
}

# Load item information
itemlabels<-read_excel("C:/Users/magnuspjo/RISE/KP Centrum för Kategoriskt Baserade Mätningar - Dokument/Kunskapsutveckling/RaschR/data/WAAQitemlabels.xls")
#responseOptions<-read_excel("C:/Users/magnuspjo/RISE/KP Centrum för Kategoriskt Baserade Mätningar - Dokument/Kunskapsutveckling/RaschR/data/PSS10itemlabels.xls", sheet = 2)
#introText<-read_excel("C:/Users/magnuspjo/RISE/KP Centrum för Kategoriskt Baserade Mätningar - Dokument/Kunskapsutveckling/RaschR/data/PSS10itemlabels.xls", sheet = 3)
```

## Background information

The Work-related Acceptance and Action Questionnaire [@bond2013] is a
measure of psychological flexibility contextualized to a workplace
setting.

The WAAQ uses 7 questions, all using the same response scale with 7
steps (ranging from *"Stämmer aldrig"* to *"Stämmer alltid"*, with
labels on all categories).

```{r}
#| label: showitems
#| tbl-cap: "Items in the Swedish WAAQ"
#| tbl-cap-location: top

itemlabels %>% 
  kbl(booktabs = T, escape = F,
      table.attr = "style='width:60%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = T,
                font_size = r.fontsize,
                fixed_thead = T) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))
  
```

### Participants

We have `r nrow(df.omit.na)` participants and their gender distribution
is shown in the table below. All participants are Swedish adults and the
dataset was collected to validate a measure of social interactions at
workplaces [@johansson2021; @johansson2021a].

```{r}
#| label: genderdistr
#| tbl-cap: "Gender distribution in sample"
#| tbl-cap-location: top

# Make table object to show gender counts and percentages
dif.gender %>%
  recode("1='Male';2='Female'") %>% 
  table() %>% 
  as_tibble() %>% 
  mutate('Percent' = (round((100 * n / sum(n)),1))) %>% 
  dplyr::rename('Gender' = '.') %>% 
  kbl(booktabs = T, escape = F, table.attr = "style='width:20%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = T,
                font_size = r.fontsize,
                fixed_thead = T) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))
                
```

### Descriptives of raw data

Responses to all items are summarized below.

```{r}
#| label: descriptives1
#| tbl-cap: "Total number of responses for all items"

df.omit.na %>% 
  pivot_longer(everything()) %>% 
  dplyr::count(value) %>% 
  mutate(percent = (100 * n / sum(n)) %>% round(digits = 1)) %>%
  rename('Response category' = 'value', 
                'Number of responses' = 'n',
                'Percent' = 'percent') %>%
  kbl(booktabs = T, escape = F, table.attr = "style='width:25%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))
```

### Descriptives - item level {.scrollable}

::: panel-tabset
### Tile plot

```{r}
#| label: descriptives2

df.omit.na %>% 
  pivot_longer(everything()) %>% 
  dplyr::count(name, value) %>% 
  ggplot(aes(x = value, y = name, fill = n)) +
  geom_tile() +
  scale_fill_viridis_c(expression(italic(n)), limits = c(0, NA)) +
  scale_x_continuous("Response", expand = c(0, 0), breaks = 0:7) + # change breaks to fit number of response categories
  ggtitle("Items") +
  theme(axis.text.x = element_text(size = 8)) +
  geom_text(aes(label=n), colour = "orange") 

```

### Barplots {.scrollable}

```{r}
#| label: alt-descriptives
#| layout-ncol: 2

# Individual barplots of raw response data
#par(mfrow = c(1,2)) # display two per row
for (i in 1:ncol(df.omit.na)) {
  barplot(table(df.omit.na[,i]), col="#8dc8c7",
          main=names(df.omit.na[i]),
          ylab="Number of responses", 
          xlab=(itemlabels[i,2]))
}

```
:::

#### Recoding responses

There are fewer than 10 responses in category 0 for all items, which is
problematic for the analysis. We will merge the two lowest categories.
For item 2 the four lowest categories will be merged.

```{r}
#| label: recode

# recode all variables in dataframe (from column 1 to the maximum number of columns)
for (i in 1:ncol(df.omit.na)) {
  df.omit.na[,i]<-recode(df.omit.na[,i],"1=0;2=1;3=2;4=3;5=4;6=5",as.factor=FALSE)
}
# then recode a single variable, q2
df.omit.na$q2<-recode(df.omit.na$q2,"1=0;2=0;3=1;4=2;5=3",as.factor=FALSE)

# make a new tile plot to review results
df.omit.na %>% 
  pivot_longer(everything()) %>% 
  dplyr::count(name, value) %>% 
  ggplot(aes(x = value, y = name, fill = n)) +
  geom_tile() +
  scale_fill_viridis_c(expression(italic(n)), limits = c(0, NA)) +
  scale_x_continuous("Response", expand = c(0, 0), breaks = 0:7) + # change breaks to fit number of response categories
  ggtitle("Items") +
  theme(axis.text.x = element_text(size = 8)) +
  geom_text(aes(label=n), colour = "orange") 

```

Some cells still have fewer than 10 responses. Let's look at the
response categories using Rasch modeling to produce ICC plots.

## Analysis of response categories

```{r}
#| label: respcat1
#| include: false

# package mirt is used to create a compact figure with all item ICCs
mirt.rasch <- mirt(df.omit.na, model=1, itemtype='Rasch') # unidimensional Rasch model
```

```{r}
#| label: respcatfig1
plot(mirt.rasch, type="trace")
# for bigger, better looking, individual figures:
#df.erm<-PCM(df.omit.na) # run PCM model, replace with RSM (rating scale) or RM (dichotomous) for other models
#plotICC(df.erm, xlim = c(-6, 8), legpos = FALSE, ylab = "Sannolikhet", xlab = "Övergripande förmåga")

```

Items 1 and 5 have issues with disordered response categories, which
need to be addressed. For both items the two lowest categories will be
merged.

### ICC plots for recoded items

```{r}
#| label: recode2
#| code-fold: true
#| layout-ncol: 2

df.omit.na$q1<-recode(df.omit.na$q1,"1=0;2=1;3=2;4=3;5=4",as.factor=FALSE)
df.omit.na$q5<-recode(df.omit.na$q5,"1=0;2=1;3=2;4=3;5=4",as.factor=FALSE)

# individual plots for those two items:
df.erm<-PCM(df.omit.na) # run PCM model, replace with RSM (rating scale) or RM (dichotomous) for other models
plotICC(df.erm, xlim = c(-6, 6), # change the theta interval to display
        legpos = FALSE, # change legpos to TRUE if you want the legend displayed 
        ylab = "Probability", xlab = "Person location/ability",
        item.subset = c("q1","q5"))
```

Response categories are working as expected, after the adjustments made.

### Floor/ceiling effects - raw data

```{r}
#| label: rawdist2
#| fig-dpi: 300

# get info on thresholds
item.estimates <- eRm::thresholds(df.erm)
item_difficulty <- item.estimates[["threshtable"]][["1"]]
item_difficulty<-as.data.frame(item_difficulty)

# all items should have lowest category 0, making 0 the lowest total score
rawMin <- 0 

# get the number of thresholds above 0, to calculate max total raw score
rawMax <- item_difficulty %>% 
  select(starts_with("Threshold")) %>% 
  pivot_longer(everything()) %>% 
  na.omit() %>% 
  count() %>% 
  pull()

# what is the lowest score in the sample?
rawMinX <- df.omit.na %>%
  mutate(rowsums = rowSums(.)) %>%
  count(rowsums) %>% 
  arrange(rowsums) %>% 
  head(1) %>% 
  pull(rowsums)
  
# if lowest score is higher than 0, we have no floor effect
if (rawMinX > 0) {
  rawMinN <- 0
} else { # if lowest score is 0, how many participants have scored 0?
rawminN <- df.omit.na %>%
  mutate(rowsums = rowSums(.)) %>%
  count(rowsums) %>% 
  arrange(rowsums) %>% 
  head(1) %>% 
  pull(n) 
}

# what is the highest score in the sample?
rawMaxX <- df.omit.na %>%
  mutate(rowsums = rowSums(.)) %>%
  count(rowsums) %>% 
  arrange(desc(rowsums)) %>% 
  head(1) %>% 
  pull(rowsums)
  
# if highest score is below max rawscore, we have no ceiling effect
if (rawMaxX < rawMax) {
  rawMaxN <- 0
} else {
rawMaxN <- df.omit.na %>%
  mutate(rowsums = rowSums(.)) %>%
  count(rowsums) %>% 
  arrange(desc(rowsums)) %>% 
  head(1) %>% 
  pull(n) 
}

# create barplot to show sum score distribution
df.omit.na %>% 
  mutate('Raw sum score' = rowSums(.)) %>% 
  pull() %>% 
  table() %>%   
  barplot(main = "Distribution of summed ordinal raw scores", 
       ylab = "Number of participants",
       xlim = c(0, rawMax),
       space = 0,
       col = RISEprimGreen)

# ceiling effect
ceiling_eff<-round(rawMaxN/nrow(df.omit.na)*100,2)
# floor effect
floor_eff<-round(rawMinN/nrow(df.omit.na)*100,2)

```

The proportion of participants responding in the lowest category on all
items (total score = 0) is `r floor_eff`%, while the proportion who
respond in the highest category on all items (total score = `r rawMax`)
is `r ceiling_eff`%.

## Dimensionality

The eRm package, which uses Conditional Maximum Likelihood (CML)
estimation, will be used primarily. For this analysis, the Partial
Credit Model will be used.

First, we look at the eigenvalues from PCA analysis of Rasch residuals.
These should be below 2.0 to support unidimensionality. If the first
eigenvalue is above 2.0, the subsequent analysis will include a
clustering analysis, using Mokken Scaling.

```{r}
#| label: dim1
#| tbl-cap: "PCA of Rasch model residuals"

df.erm<-PCM(df.omit.na) # run PCM model, replace with RSM (rating scale) or RM (dichotomous) for other models
# get estimates, code borrowed from https://bookdown.org/chua/new_rasch_demo2/PC-model.html
item.estimates <- eRm::thresholds(df.erm)
item_difficulty <- item.estimates[["threshtable"]][["1"]]
item_difficulty<-as.data.frame(item_difficulty)
item.se <- item.estimates$se.thresh
person.locations.estimate <- person.parameter(df.erm)
item.fit <- eRm::itemfit(person.locations.estimate)
std.resids <- item.fit$st.res
# PCA of Rasch residuals
pca <- pca(std.resids, nfactors = ncol(df.omit.na), rotate = "oblimin")
# create table with top 5 eigenvalues
pca$values %>%
  round(2) %>%
  head(5) %>% 
  as_tibble() %>% 
  rename('Eigenvalues' = 'value') %>% 
  kbl(booktabs = T, escape = F, table.attr = "style='width:25%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = T,
                font_size = r.fontsize,
                fixed_thead = F) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

```

Since the eigenvalues support unidimensionality, we move on to look at
item fit to the Rasch PCM.

### Item fit

```{r}
#| label: modelfit1
#| tbl-cap: "Item fit"
#| tbl-cap-location: top
#| layout-ncol: 2

### zstd is inflated with large samples (N > 500). Reduce sample size to jz and 
### run analysis yz random samples to get average ZSTD (defined in r setup chunk)
# outfitZ<-c()
# infitZ<-c()
# for (i in 1:yz) {
#   df.new <- df.omit.na[sample(1:nrow(df.omit.na), jz), ]
#   df.new <- na.omit(df.new)
#   df.z <- PCM(df.new)
#   ple <- person.parameter(df.z)
#   item.fit.z <- eRm::itemfit(ple)
#   outfitZ<-cbind(outfitZ,item.fit.z$i.outfitZ)
#   infitZ<-cbind(infitZ,item.fit.z$i.infitZ)
# }
# item.fit.table<-as.data.frame(cbind(item.fit$i.outfitMSQ, item.fit$i.infitMSQ, rowMeans(outfitZ), rowMeans(infitZ)))

# if you have a smaller sample, use this line instead of running the loop above
item.fit.table<-as.data.frame(cbind(item.fit$i.outfitMSQ, item.fit$i.infitMSQ, item.fit$i.outfitZ, item.fit$i.infitZ))
colnames(item.fit.table)<-c("OutfitMSQ", "InfitMSQ", "OutfitZSTD", "InfitZSTD")

# round the numbers to 3 decimal digits
item.fit.table <- item.fit.table %>%
    mutate(across(where(is.numeric), round, 3))

# create table that highlights cutoff values in red
item.fit.table %>% 
  mutate(OutfitZSTD = cell_spec(OutfitZSTD, color = ifelse(OutfitZSTD < zstd_min, "red",
                                                     ifelse(OutfitZSTD > zstd_max, "red", "black")))) %>%
  mutate(InfitZSTD = cell_spec(InfitZSTD, color = ifelse(InfitZSTD < zstd_min, "red",
                                                     ifelse(InfitZSTD > zstd_max, "red", "black")))) %>%
  mutate(OutfitMSQ = cell_spec(OutfitMSQ, color = ifelse(OutfitMSQ < msq_min, "red",
                                                     ifelse(OutfitMSQ > msq_max, "red", "black")))) %>%
  mutate(InfitMSQ = cell_spec(InfitMSQ, color = ifelse(InfitMSQ < msq_min, "red",
                                                     ifelse(InfitMSQ > msq_max, "red", "black")))) %>%
  kbl(booktabs = T, escape = F) %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
#  column_spec(c(2:3), color = "red") %>% 
#  row_spec(3:5, bold = T, color = "white", background = "lightblue") %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

# print the items in a separate table that should appear beside the item fit table
itemlabels %>% 
  filter(itemnr %in% names(df.omit.na)) %>% 
  kbl(booktabs = T, escape = F,
      table.attr = "style='width:60%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = T,
                font_size = r.fontsize,
                fixed_thead = T) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

```

Values in red are beyond the pre-set cutoff values. We'll leave them for
now.

### Residual correlations

A correlation matrix is created based on the Rasch model residuals for
each item.

The average correlation is calculated, and item pairs that correlate
more than the pre-set cutoff value of `r loc_dep` above the average
correlation are indicated in red in the table below.

```{r}
#| label: locdeps1
#| include: false

mirt.rasch <- mirt(df.omit.na, model=1, itemtype='Rasch') # unidimensional Rasch model
resid=residuals(mirt.rasch, type="Q3", digits=2)
diag(resid) <- NA # make the diagonal of correlation matrix NA instead of 1
dyn.cutoff<-mean(resid, na.rm=T) + loc_dep # create variable indicating dynamic cutoff at 0.3 above average
resid<-as.data.frame(resid)
for (i in 1:ncol(resid)) {
  resid[,i]<-round(resid[,i],2)
}

```

```{r}
#| label: locdeps1tbl
# tbl-cap: "Residual correlations"
#| layout-ncol: 2

resid[upper.tri(resid)]<-NA # remove duplicate values in upper triangle

resid %>% 
   mutate(across(everything(), ~ cell_spec(.x, color = case_when(.x >= dyn.cutoff ~ "red", TRUE ~ "black")))) %>%  
  kbl(booktabs = T, escape = F, 
      table.attr = "style='width:70%;'") %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

itemlabels %>% 
  filter(itemnr %in% names(df.omit.na)) %>% 
  kbl(booktabs = T, escape = F,
      table.attr = "style='width:60%;'") %>%
  # options for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "center",
                full_width = T,
                font_size = r.fontsize,
                fixed_thead = T) %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

ldcut<-round(dyn.cutoff,3)

```

Items 1 and 3 are correlated above the relative cutoff value
(`r ldcut`). Looking at the item descriptions, both are
related to working efficiently when being worried in general (item 3) or
being worried about personal problems. As we saw in the section on item
fit, item 3 had quite low item fit values. This is a common issue with
broad questions.

It would be reasonable to eliminate either question 1 or 3, since they
clearly overlap in content. We will leave this for an exercise.

## DIF analysis

Our sample is underpowered to conduct a proper DIF-analysis of any of
the demographic variables available (gender/age). N \> 250 per subgroup
would be ideal, and N \> 150 minimal.

For the sake of practice, we will go through the steps in setting up and
conducting a DIF analysis anyway.

We will use the Rasch Tree tool [@strobl2013], which makes it possible
to evaluate interaction DIF, if sample size allows. For this analysis,
we will only investigate gender effects on DIF.

```{r}
#| label: dif1
# fig-cap: "DIF analysis of gender"
# fig-cap-location: top

# set up the DIF analysis
df.tree <- data.frame(matrix(ncol = 0, nrow = nrow(df.omit.na))) # we need to make a new dataframe
df.tree$difdata <- as.matrix(df.omit.na) # containing item data in a nested dataframe
# and DIF variables:
df.tree$gender<-dif.gender
pctree.out<-pctree(difdata ~ gender, data = df.tree)
plot(pctree.out)

```

No DIF was found in our small sample.

## Targeting

```{r}
#| label: wrightm1
# fig-cap: Modified Wright map
# fig-cap-location: top
#| fig-alt: "Three figures with the same scale on top of each other. At the top is a histogram of Person Locations, with a dotted line and gray field indicating mean/SD. In the middle is a similar histogram with Item Thresholds. At the bottom is a figure showing the individual item thresholds."

item.locations<-item_difficulty[,2:ncol(item_difficulty)]
names(item.locations) <- paste0("T", c(1:ncol(item.locations))) #re-number items
itemloc.long <- item.locations %>%
  rownames_to_column() %>% 
  dplyr::rename(names = 'rowname') %>%
  pivot_longer(cols=starts_with('T'), 
               names_to ='thresholds', 
               values_to = 'par_values' )

# make plot with each items thresholds shown as dots
p1=ggplot(itemloc.long, aes(x = names, y = par_values, label = thresholds, color = names)) +
  geom_point() + 
  geom_text(hjust = 1.1, vjust = 1) + 
  ylab('Location (logit scale)') + 
  xlab('Items') + 
  scale_y_continuous(limits = c(-5,6), breaks = scales::pretty_breaks(n = 10)) + 
  theme_bw() + 
  theme(legend.position = 'none') + 
  coord_flip()

### create df for ggplot histograms
# person locations
thetas<-as.data.frame(person.locations.estimate$theta.table)
pthetas<-thetas$`Person Parameter`
# item locations
thresholds<-c()
for (i in 2:ncol(item_difficulty)) {
  thresholds<-c(thresholds,item_difficulty[,i])
}
### items and persons in the same variable
#create data frame with 0 rows and 3 columns
df.locations <- data.frame(matrix(ncol = 2, nrow = 0))
#provide column names
colnames(df.locations) <- c('type', 'locations')
# change type of data
df.locations$type<-as.character(df.locations$type)
df.locations$locations<-as.numeric(df.locations$locations)
# insert labels in accurate amounts (N+items)
nper<-nrow(df.omit.na)
nperp<-nper+1
nthr<-length(thresholds)+nper
df.locations[1:nper,1]<-paste0("Persons")
df.locations[nperp:nthr,1]<-paste0("Item thresholds")
# insert data from vectors with thetas and thresholds
df.locations$locations<-c(pthetas,thresholds)
# change type to class factor
df.locations$type<-as.factor(df.locations$type)

# get mean/SD for item/person locations
pi.locations <- data.frame(matrix(ncol = 3, nrow = 3))

#
item.mean <- round(mean(item_difficulty$Location),2)
item.sd <- round(sd(item_difficulty$Location),2)
item.thresh.sd <- item_difficulty %>% 
  select(starts_with("Threshold")) %>% 
  pivot_longer(everything()) %>% 
  pull() %>% 
  na.omit() %>% sd() %>% round(2)
person.mean <- round(mean(pthetas),2)
person.sd <- round(sd(pthetas),2)
#provide column names
colnames(pi.locations) <- c('','Mean', 'SD')
pi.locations[1,1] <- "Items"
pi.locations[1,2] <- round(mean(item_difficulty$Location),2)
pi.locations[1,3] <- round(sd(item_difficulty$Location),2)
pi.locations[2,1] <- "Item thresholds"
pi.locations[2,2] <- round(mean(item_difficulty$Location),2)
pi.locations[2,3] <- item.thresh.sd
pi.locations[3,1] <- "Persons"
pi.locations[3,2] <- round(mean(pthetas),2)
pi.locations[3,3] <- round(sd(pthetas),2)

# Person location histogram
p2<-ggplot() + 
  geom_histogram(data=subset(df.locations, type=="Persons"), 
                 aes(locations, fill="Persons", y= ..count..)) +
  xlab('') +
  ylab('Persons') +
  scale_x_continuous(limits = c(-5,6), breaks = scales::pretty_breaks(n = 10)) + 
  geom_vline(xintercept = person.mean, color = RISEcompGreenDark, linetype = 2) +
  annotate("rect", ymin = 0, ymax = Inf, xmin = (person.mean-person.sd), xmax = (person.mean+person.sd), alpha = .2) +
  geom_text(hjust = 1.1, vjust = 1) +
  theme_bw() +
  theme(legend.position = 'none',
        text=element_text(family = "sans"))

# Item Threshold location histogram
p3 <- ggplot() +
  geom_histogram(data=subset(df.locations, type=="Item thresholds"), 
                 aes(locations, y= ..count..)) + 
  xlab('') +
  ylab('Thresholds') +
  scale_x_continuous(limits = c(-5,6), breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous() +
  scale_y_reverse() +
  geom_vline(xintercept = item.mean, color = RISEcompGreenDark, linetype = 2) +
  annotate("rect", ymin = 0, ymax = Inf, xmin = (item.mean-item.thresh.sd), xmax = (item.mean+item.thresh.sd), alpha = .2) +
  geom_text(hjust = 1.1, vjust = 1) +
  theme_bw() +
  theme(legend.position = 'none')

# combine plots together to create Wright map, and let the individual item threshold plot have some more space
plot_grid(p2,p3,p1, labels=NULL, nrow = 3, align ="hv", rel_heights = c(1,1,1.3))

```

### Person/item location descriptives

```{r}
#| label: PIlocation
#| tbl-cap: "Person/Item locations"

pi.locations %>% 
  kbl(booktabs = T, escape = F) %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
#  column_spec(c(2:3), color = "red") %>% 
#  row_spec(3:5, bold = T, color = "white", background = "lightblue") %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

```

### Reliability

"Test information" shows the collective information based on the
combination of all items into a unidimensional scale.

```{r}
#| label: tif1

# we need to make a new dataframe for the test information plot/curve
psimatrix <- data.frame(matrix(ncol = 2, nrow = 1001)) 
names(psimatrix) <- c("psY","psX")
# this gets 1001 "dots" for the scale information variable y
psimatrix$psY <- test_info(df.erm, seq(-6, 6, length.out = 1001L)) 
# this is the x variable in the TIF figure
psimatrix$psX <- seq(-6, 6, length.out = 1001L)

ggplot(psimatrix) + 
  geom_point(aes(x=psX, y=psY), size = 0.1, color = dot_color) +
  geom_hline(yintercept = 3.33, color = cutoff_line, linetype = 2, size = 0.5) +
  geom_hline(yintercept = 5, color = cutoff_line, linetype = 2, size = 0.7) + 
  scale_y_continuous(breaks=seq(0, 8, by = 1)) +
  scale_x_continuous(breaks=seq(-6, 6, by = 1)) +
  labs(title = "Test information", x = "Logits", y = "Test information") +
  theme(
  panel.background = element_rect(fill = backg_color,
                                colour = backg_color,
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white")
  )

# eRm functions to create TIF plot:
#plotINFO(df.erm, type = "test") # show only test information
#abline(h = 3.33, col = "chartreuse") # indicate TIF 3.33 = PSI 0.70
#abline(h = 5, col = "chartreuse3") # indicate TIF 5 = PSI 0.80

```

The horizontal lines at TIF 3.33 and 5.0 indicate the conventional lower
cutoff for acceptable reliability of 0.7 and 0.8, respectively.

#### Item information
"Item information" shows individual curves indicating the amount of
information contributed by each item.

```{r}
#| label: iteminf

plotINFO(df.erm, type = "item", legpos = "topleft") # get item information curves
```

### Floor/ceiling effects - test parameters

```{r}
#| label: floorc1

# check if TIF goes above 3.3
peak.tif <- psimatrix %>% slice(which.max(psY)) %>% select(psY) %>% pull()

if (peak.tif > 3.32) {
# now find where the cutoff points are for 3.33 on the theta (x) variable 
# this provides the highest and lowest value into two variables
psep_min <- psimatrix %>% filter(psX < 0) %>% slice(which.min(abs(psY - 3.33))) %>% select(psX) %>% pull()
psep_max <- psimatrix %>% filter(psX > 0) %>%  slice(which.min(abs(psY - 3.33))) %>% select(psX) %>% pull()
} else {
  psep_min = 0
  psep_max = 0
}

# If peak.tif is below 3.33 (= PSI 0.7), the text section after this code chunk will not work, and you'll have to replace it with something appropriate


# calculate how many participants cross the cutoffs
nCeilingRel<-length(which(pthetas > psep_max))
nFloorRel<-length(which(pthetas < psep_min))
nWithinRel<-(length(pthetas)-(nCeilingRel+nFloorRel))
PSI<-SepRel(person.locations.estimate)

### if there are large gaps in targeting, it is useful to calculate how many are affected:
# calculate how many respondents are in the gap area
# nWithinGap<-c()
# nWithinGap[pthetas > psep_max  & pthetas < psep_min] <- 1
# 
# pGap <- nWithinGap %>%
#   as.integer() %>%
#   table() %>%
#   as.tibble() %>%
#   dplyr::select(n) %>%
#   pull()

# Retrieve the lowest and highest item thresholds into vector variables

min_thresh <- df.locations %>% 
  filter(type == "Item thresholds") %>% 
  arrange(locations) %>% 
  slice(1) %>% 
  pull()

max_thresh <- df.locations %>% 
  filter(type == "Item thresholds") %>% 
  arrange(desc(locations)) %>% 
  slice(1) %>% 
  pull()

# calculate how many participants cross the cutoffs
nCeilingThresh<-length(which(pthetas > max_thresh))
nFloorThresh<-length(which(pthetas < min_thresh))

```

Test information shows that WAAQ reaches reliability above 0.7 in the
interval from `r psep_min` til `r psep_max` logits.
**`r round(nWithinRel/length(pthetas)*100,2)`%** of participants are
located within that area.

**`r round(nCeilingRel/length(pthetas)*100,1)`%** have person locations
above the cutoff for reliability 0.7 and
**`r round(nFloorRel/length(pthetas)*100,1)`%** are below.

Person separation as a point estimate is **`r round(PSI$sep.rel,3)`**.

**`r round(nCeilingThresh/length(pthetas)*100,1)`%** have person
locations above the highest item threshold (`r round(max_thresh,2)`) and
**`r round(nFloorThresh/length(pthetas)*100,1)`%** are below the lowest
item threshold (`r round(min_thresh,2)`).

### Person fit

```{r}
#| label: personfit1
# fig-cap: "Person locations and Person infit ZSTD"

# seems like the person map is reversed? compare to histogram 
person.fit <- eRm::personfit(person.locations.estimate)
thetas2<-as.data.frame(person.locations.estimate$theta.table)

nPfit <- length(person.fit$p.infitZ)
nCeilingPfit<-length(which(person.fit$p.infitZ > 2))
nFloorPfit<-length(which(person.fit$p.infitZ < -2))
nPgoodfit<-(nPfit-(nCeilingPfit+nFloorPfit))

hist(person.fit$p.infitZ, col = RISEprimGreen, xlim = c(-4,6), xlab = "Person infit ZSTD", main = "Histogram of Person infit ZSTD")

# check whether there are excluded observations, and if found, adjust thetas2 df
if (length(person.fit$excl_obs_num) > 0L) {
  thetas2[person.fit$excl_obs_num,] <- NA
  thetas2 <- na.omit(thetas2)
}
df.pfit <- data.frame(matrix(ncol = 2, nrow = nrow(thetas2)))
#provide column names
colnames(df.pfit) <- c('Person locations', 'Person infit ZSTD')
df.pfit$`Person locations` <- thetas2$`Person Parameter`
df.pfit$`Person infit ZSTD` <- person.fit$p.infitZ

cutoff_line <- RISEprimRed
dot_color <- "black"
backg_color <- RISEprimGreenLight
```

#### Person location and infit ZSTD

```{r}
#| label: plocinfz

df.pfit %>% 
  ggplot(aes(x=`Person infit ZSTD`, y=`Person locations`, label="")) +
  geom_point(size = 1, color = dot_color) +
  geom_vline(xintercept = -2, color = cutoff_line, linetype = 2, size = 0.7) +
  geom_vline(xintercept = 2, color = cutoff_line, linetype = 2, size = 0.7) + 
  scale_y_continuous(breaks=seq(-5, 5, by = 1)) +
  scale_x_continuous(breaks=seq(-5, 7, by = 1)) +
  theme(
  panel.background = element_rect(fill = backg_color,
                                colour = backg_color,
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white")
  )

```

**`r round(nCeilingPfit/nPfit*100,1)`%** have person infit t above the
cutoff 2.0 and **`r round(nFloorPfit/nPfit*100,1)`%** are below -2.0.
Thus, **`r round(nPgoodfit/nPfit*100,1)`%** are within +/- 2 infit ZSTD.

## Item parameters

```{r}
#| label: itemparams
# tbl-cap: "Item thresholds and locations"

item_difficulty %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  relocate(Location, .after = last_col()) %>% 
  kbl(booktabs = T, escape = F) %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
#  column_spec(c(2:3), color = "red") %>% 
#  row_spec(3:5, bold = T, color = "white", background = "lightblue") %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))

```

## Ordinal to interval transformation table

```{r}
#| label: ordintTr
#| include: false
ple<-as.data.frame(print(person.locations.estimate))
rownames(ple)<-NULL
colnames(ple)<-c("Ordinal sum score","Logits","SE (Logits)")
ple[,2]<-round(ple[,2],2)
ple[,3]<-round(ple[,3],2)

ple$'Interval score (0-100)'<-round(scales::rescale(ple[,2], to = c(0,100)),1) # set 0,100 to desired range
ple$'SE (0-100)'<-round((ple$`SE (Logits)`/(0.83/8)),1) # approximated from looking at the table
ple$'MU (0-100)'<-(ple$'SE (0-100)')*2
```

```{r}
#| label: ordintTbl
#| tbl-cap: "Ordinal-interval transformation table"
#| layout-ncol: 2

ple %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  kbl(booktabs = T, escape = F) %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"), 
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
#  column_spec(c(2:3), color = "red") %>% 
#  row_spec(3:5, bold = T, color = "white", background = "lightblue") %>% 
  column_spec(1, bold = T) %>% 
  kable_classic(html_font = "Lato") %>% 
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down")) %>% 
  footnote(general = "SE = Standard Error. Multiply by 2 for MU = Measurement Uncertainty")
```

```{r}
#| label: ordintPlot
# fig-cap: "Ordinal/interval relationship"
#| include: false
#| eval: false

# plot with SE, code borrowed from https://www.benjaminbell.co.uk/2019/04/how-to-add-error-bars-in-r.html
x<-ple$`Ordinal sum score`
#y<-ple$Intervallpoäng
y<-ple$Logits
plot(x,y)
y.se<-ple$`SE (Logits)`
arrows(x0=x, y0=y-y.se, x1=x, y1=y+y.se, code=3, angle=90, length=0.1)
```

## Construct alley infit

```{r}
#| label: prepconall
#| include: false

item.fit.table$Location <- item_difficulty$Location

```

```{r}
#| label: conallinf
# fig-cap: "InfitZSTD and item location"

cutoff_line <- RISEprimRed
dot_color <- "black"
backg_color <- RISEprimGreenLight

item.fit.table %>% 
  rownames_to_column() %>% 
  ggplot(aes(x=InfitZSTD, y=Location, label = rowname)) +
  geom_point(size = 3, color = dot_color) +
  geom_vline(xintercept = -2, color = cutoff_line, linetype = 2) +
  geom_vline(xintercept = 2, color = cutoff_line, linetype = 2) + 
  geom_text(hjust=1.5) + 
  theme(
  panel.background = element_rect(fill = backg_color,
                                colour = backg_color,
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white")
  )

```

## Construct alley outfit

```{r}
#| label: conalloutf
# fig-cap: "OutfitZSTD and item location"

item.fit.table %>% 
  rownames_to_column() %>% 
  ggplot(aes(x=OutfitZSTD, y=Location, label = rowname)) +
  geom_point(size = 3, color = dot_color) +
  geom_vline(xintercept = -2, color = cutoff_line, linetype = 2) +
  geom_vline(xintercept = 2, color = cutoff_line, linetype = 2) + 
  geom_text(hjust=1.5) + 
  theme(
  panel.background = element_rect(fill = backg_color,
                                colour = backg_color,
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white")
  )

```

## Loadings vs item locations

```{r}
#| label: conallpca
# fig-cap: "PCA of residuals first contrast loadings and item location"
#| fig-dpi: 300

pca2<-prcomp(std.resids)
pcaloadings <- as.data.frame(pca2$rotation)
pcaloadings$Location <- item.fit.table$Location

pcaloadings %>% 
  rownames_to_column() %>% 
  ggplot(aes(x=Location, y=PC1, label = rowname)) +
  geom_point(size = 3, color = dot_color) +
  xlim(-2, 2) +
  ylim(-2, 2) +
  ylab("Loadings") +
  xlab("Item location") +
  geom_vline(xintercept = 0, color = cutoff_line, linetype = 2) +
  geom_hline(yintercept = 0, color = cutoff_line, linetype = 2) + 
  geom_text(hjust=1.4, vjust=0.6) +
  theme(
  panel.background = element_rect(fill = backg_color,
                                colour = backg_color,
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white")
  )

```

## Software used

```{r}
#| label: packagesv


pkgs <- cite_packages(cite.tidyverse = TRUE, 
                      output = "table",
                      bib.file = "grateful-refs.bib",
                      include.RStudio = TRUE)

pkgs %>%
  kbl(booktabs = T, escape = F,
      table.attr = "style='width:80%;'") %>%
  # bootstrap options are for HTML output
  kable_styling(bootstrap_options = c("striped", "hover"),
                position = "left",
                full_width = F,
                font_size = r.fontsize,
                fixed_thead = T) %>% # when there is a long list in the table
  column_spec(1, bold = T) %>%
  kable_classic(html_font = "Lato") %>%
  # latex_options are for PDF output
  kable_styling(latex_options = c("striped","scale_down"))
```

## Exercises

1.  Make a table of partipant age distribution
2.  Eliminate either item 1 or 3 and check the effects on reliability
3.  Run a DIF analysis using age as DIF variable
    -   first add the dif.age variable in the setup chunk
    -   then duplicate the DIF chunk and modify code

## References
